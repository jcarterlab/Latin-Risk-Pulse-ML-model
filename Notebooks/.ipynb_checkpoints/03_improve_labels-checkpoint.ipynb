{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad0583d6-ce4f-4a6b-90e2-b5c491bab49c",
   "metadata": {},
   "source": [
    "# 03) Improve labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179755f3-9933-450c-85c2-3f3462b80b51",
   "metadata": {},
   "source": [
    "A key problem with the headlines data is that it was collected by scrapping online news sources, keyword matching and then feeding the keyword matches to Google Gemini for labelling. Because the keyword matching proccess likely missed risk headlines, the default non-risk category probably contains a significant amount of mislabelled data. To tackle this problem, a regression model is trained on half of the data at a time to generate predictions for the other half's non-risk headlines. A slice of low probability headlines from each half is kept, eliminating many false negatives and also tackling the class imbalance problem (see notebook 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df94d6-83db-4396-8697-9615ce4ebf8a",
   "metadata": {},
   "source": [
    "## Read-in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a81d4ae-639e-4e61-802b-7b339ca3a8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.7K Spanish headlines\n",
      "12.7K Portuguese headlines\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read-in data\n",
    "df = pd.read_csv('../Data/original_headlines.csv', encoding='utf-8')\n",
    "\n",
    "# include only spanish \n",
    "spanish_df = df[df.country.isin(['Argentina', 'Colombia', 'Mexico'])].reset_index(drop=True)\n",
    "print(str(round(len(spanish_df)/1000, 1)) + 'K Spanish headlines')\n",
    "\n",
    "# include only portuguese \n",
    "portuguese_df = df[df.country == 'Brazil'].reset_index(drop=True)\n",
    "print(str(round(len(portuguese_df)/1000, 1)) + 'K Portuguese headlines')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb259b58-b2bb-41d5-9e8f-16cd6daffba4",
   "metadata": {},
   "source": [
    "## Remove duplicates & thumbnails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a3559-f4ba-47aa-9288-51ce2f79e452",
   "metadata": {},
   "source": [
    "Headlines containing the word thumbnail are normally videos which cannot be scraped. The presence of many of these with a similar format in our non-risk headlines data will add little variety and therefore value to our dataset so therefore they are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa294472-f20c-4ad3-bc71-c140c9036403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "spanish_df.drop_duplicates(subset='headline', inplace=True)\n",
    "portuguese_df.drop_duplicates(subset='headline', inplace=True)\n",
    "\n",
    "# remove thumbnails\n",
    "spanish_df = spanish_df[~spanish_df['headline'].str.lower().str.contains('thumbnail', na=False)]\n",
    "portuguese_df = portuguese_df[~portuguese_df['headline'].str.lower().str.contains('thumbnail', na=False)]\n",
    "\n",
    "# reset index\n",
    "spanish_df.reset_index(drop=True, inplace=True)\n",
    "portuguese_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a4cfdf-febc-45df-9704-eb429daa4e94",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2156dd2c-0fd0-40f3-b6b4-078b8235a199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jack-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "spanish_stop_words = set(stopwords.words('Spanish'))\n",
    "portuguese_stop_words = set(stopwords.words('Portuguese'))\n",
    "\n",
    "# common text cleaning techniques\n",
    "def clean_text(text, language):\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation + '¡¿'))\n",
    "\n",
    "    if language=='Spanish':\n",
    "        text = ' '.join([word for word in text.split() if word not in spanish_stop_words])\n",
    "    elif language=='Portuguese':\n",
    "        text = ' '.join([word for word in text.split() if word not in portuguese_stop_words])\n",
    "    \n",
    "    return text\n",
    "\n",
    "spanish_df['headline'] = [clean_text(x, 'Spanish') for x in spanish_df['headline']]\n",
    "portuguese_df['headline'] = [clean_text(x, 'Portuguese') for x in portuguese_df['headline']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cde4337-0ee8-441c-ad72-c0a22eff56f6",
   "metadata": {},
   "source": [
    "## Split dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad3760-0778-4cd1-a8d4-56e4afc09132",
   "metadata": {},
   "source": [
    "The data is split into two sets randomly so a model can be trained on each and used to predict headlines for the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac502d8e-2ee1-4945-adbb-08f875793708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "# randomly split a dataframe into 2 equal size groups \n",
    "def split_dataframes(df):\n",
    "    population = list(range(len(df)))\n",
    "    half_headlines = int(np.floor(len(population) / 2))\n",
    "    random_samples = random.sample(population, half_headlines)\n",
    "    return df.loc[random_samples,:].reset_index(drop=True), df.loc[~df.index.isin(random_samples), :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827ab7a5-0013-4c5a-9211-43c3a4740fbb",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a650657-5a11-4fa9-81cb-8563d834430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# vectorizes data and fits a model \n",
    "def fit_model(df):\n",
    "    X, y = df.headline, [int(pd.notna(x)) for x in df.risk_type]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_tfidf = vectorizer.fit_transform(X)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(train_tfidf, y)\n",
    "    return vectorizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aab4d7d-7f58-4536-acd3-c63e974f3da4",
   "metadata": {},
   "source": [
    "## Add predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c962ca39-2012-4ba9-8924-249aa61d0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns predictions as binary decisions and probabilities\n",
    "def predict_headlines(df, vectorizer, model):\n",
    "    tfidf_vectors = vectorizer.transform(df.headline)\n",
    "    y_preds = model.predict(tfidf_vectors)\n",
    "    y_pred_prob = [np.mean(model.predict_proba(x)[:, 1]) for x in tfidf_vectors]\n",
    "    return y_preds, y_pred_prob\n",
    "\n",
    "# adds the predictions for each half of the data to their respective dfs\n",
    "def add_predictions_to_df(primary_df, secondary_df):\n",
    "    vectorizer, model = fit_model(secondary_df)\n",
    "    y_preds, y_pred_prob = predict_headlines(primary_df, vectorizer, model)\n",
    "    primary_df['y_pred'], primary_df['y_pred_prob'] = y_preds, y_pred_prob\n",
    "    primary_df.sort_values('y_pred_prob', ascending=False, inplace=True)\n",
    "    return primary_df\n",
    "\n",
    "# returns a headlines dataframe along with their predictions for veiwing\n",
    "# this is useful so we can set appropriate upper and lower limits for the \n",
    "# slice of low probability non-risk headlines we will select later on\n",
    "def view_headline_preds(df, language):\n",
    "    df_1, df_2 = split_dataframes(df) \n",
    "    df_1, df_2 = add_predictions_to_df(df_1, df_2), add_predictions_to_df(df_2, df_1)\n",
    "    #df_1, df_2 =  drop_non_risk_headlines(df_1, language), drop_non_risk_headlines(df_2, language)\n",
    "    return df_1, df_2\n",
    "\n",
    "spanish_df_1, spanish_df_2 = view_headline_preds(spanish_df, 'Spanish')\n",
    "portuguese_df_1, portuguese_df_2 = view_headline_preds(portuguese_df, 'Portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860a16e5-01d9-4381-82f2-e8cbbf17f617",
   "metadata": {},
   "source": [
    "## Find false negatives threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c34fd48-344e-48e3-bdb8-7a9f97de61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function shows the non-risk labelled headlines ordered\n",
    "# from the lowest to highest probability score. An ml practitioner \n",
    "# can use this to find the appropriate threshold for which there are \n",
    "# few false negatives in the data...\n",
    "def view_nonrisk_highest_rows(df, start, end):\n",
    "    temp_df = df.loc[pd.isna(df.risk_type)].sort_values('y_pred_prob')\n",
    "    print('Non-risk headlines: ' + str(len(temp_df)))\n",
    "    print()\n",
    "    selected_index_df = temp_df.loc[pd.isna(temp_df.risk_type)].iloc[start:end, :]\n",
    "    for i in range(len(selected_index_df)):\n",
    "        print(str(selected_index_df.index[i]) + ':   ' + selected_index_df.headline.values[i])\n",
    "\n",
    "#view_nonrisk_highest_rows(spanish_df_2, 6950, 7000)\n",
    "#view_nonrisk_highest_rows(portuguese_df_1, 1850, 1900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa88004-8879-4db2-b99e-1e532e355adf",
   "metadata": {},
   "source": [
    "## Drop high score headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f024a71-a4b9-4064-a061-10fea3991bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops a number of non-risk headlines based on their prediction scores\n",
    "# keeps 7,000 headlines with low predictions\n",
    "# the lowest 3,000 are not included as these are mostly youtube videos and provide little variety\n",
    "def drop_non_risk_headlines(df, language):        \n",
    "    non_risk_df = df.loc[pd.isna(df.risk_type)]\n",
    "    risk_df = df.loc[~pd.isna(df.risk_type)]\n",
    "    \n",
    "    if language=='Spanish':\n",
    "        lower_limit, upper_limit = 0, 7000\n",
    "    elif language=='Portuguese':\n",
    "        lower_limit, upper_limit = 0, 1900\n",
    "        \n",
    "    low_score_non_risk = non_risk_df.iloc[(len(non_risk_df)-upper_limit):(len(non_risk_df)-lower_limit),:]\n",
    "    return pd.concat([risk_df, low_score_non_risk])\n",
    "\n",
    "# creates a filtered dataframe combining both halfs of the data \n",
    "# after dropping headlines with high predictions\n",
    "def create_filtered_df(df_1, df_2, language):\n",
    "    df_1, df_2 =  drop_non_risk_headlines(df_1, language), drop_non_risk_headlines(df_2, language)\n",
    "    return pd.concat([df_1, df_2])\n",
    "\n",
    "filtered_spanish_df = create_filtered_df(spanish_df_1, spanish_df_2, language='Spanish')\n",
    "filtered_portuguese_df = create_filtered_df(portuguese_df_1, portuguese_df_1, language='Portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7736f3-fa01-494f-97b0-c56ae9bf9e9b",
   "metadata": {},
   "source": [
    "## Create train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb3ae042-787f-4062-a379-53c4c7965c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# returns a train test split\n",
    "def split_data(df, test_size=0.25):\n",
    "    X = df.headline\n",
    "    y = [int(pd.notna(x)) for x in df.risk_type]\n",
    "    return train_test_split(X, y, test_size=test_size, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb49fe-5010-4b12-bd75-c8ba47173e18",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6738b006-d7ac-46ce-b84d-659a6fdae24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# evaluates the model's performance and prints the results\n",
    "def evaluate_model(model, X_test_tfidf, y_test):\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    y_pred_prob = model.predict_proba(X_test_tfidf)[:, 1] \n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ce510-3194-487d-be73-1e6dd336f4ee",
   "metadata": {},
   "source": [
    "## New headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d165b8f4-a8be-43cb-a709-4d5095b1ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses totally new headlines to evaluate the data - this\n",
    "# is necessary because the filtered dfs now include only \n",
    "# easier non-risk headlines with low prediction scores\n",
    "def get_new_df(language):\n",
    "    new_df = pd.read_csv('../Data/headlines.csv', encoding='utf-8')\n",
    "    new_df['headline'] = [clean_text(x) for x in new_df['headline']]\n",
    "    \n",
    "    difference = len(new_df) - len(df)\n",
    "    new_headlines = new_df.iloc[(len(new_df)-difference-1):len(new_df),:]\n",
    "\n",
    "    if language=='Spanish':\n",
    "        new_headlines = new_headlines.loc[new_headlines.country.isin(['Argentina', 'Colombia', 'Mexico'])]\n",
    "    elif language=='Portuguese':\n",
    "        new_headlines = new_headlines.loc[new_headlines.country == 'Brazil']\n",
    "    \n",
    "    new_headlines.reset_index(drop=True, inplace=True)\n",
    "    print(str(round(len(new_headlines)/1000, 2)) + 'K never before seen headlines')\n",
    "    print()\n",
    "    return new_headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2774c43-ef02-4c6d-970e-f1abfaf243fc",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55d5cfb4-7b65-44e0-bafe-fc1e01327d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Spanish ***\n",
      "\n",
      "3.34K never before seen headlines\n",
      "\n",
      "Accuracy: 0.842326139088729\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.90      2884\n",
      "           1       0.46      0.87      0.60       452\n",
      "\n",
      "    accuracy                           0.84      3336\n",
      "   macro avg       0.72      0.85      0.75      3336\n",
      "weighted avg       0.91      0.84      0.86      3336\n",
      "\n",
      "\n",
      "\n",
      "*** Portuguese ***\n",
      "\n",
      "0.53K never before seen headlines\n",
      "\n",
      "Accuracy: 0.7518796992481203\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.75      0.82       401\n",
      "           1       0.50      0.76      0.60       131\n",
      "\n",
      "    accuracy                           0.75       532\n",
      "   macro avg       0.70      0.76      0.71       532\n",
      "weighted avg       0.81      0.75      0.77       532\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluates a filtered dataset against totally new headlines\n",
    "def check_results(train_df, language):\n",
    "    print()\n",
    "    print('*** ' + language + ' ***')\n",
    "    print()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_data(train_df, test_size=0.1)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    new_headlines = get_new_df(language)\n",
    "    \n",
    "    X_test_tfidf = vectorizer.transform(new_headlines.headline)\n",
    "    y_test = [int(pd.notna(x)) for x in new_headlines.risk_type]\n",
    "    \n",
    "    evaluate_model(model, X_test_tfidf, y_test)\n",
    "\n",
    "check_results(filtered_spanish_df, language='Spanish')\n",
    "check_results(filtered_portuguese_df, language='Portuguese')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
