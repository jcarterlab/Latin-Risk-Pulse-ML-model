{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad0583d6-ce4f-4a6b-90e2-b5c491bab49c",
   "metadata": {},
   "source": [
    "# 03) Improve label quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179755f3-9933-450c-85c2-3f3462b80b51",
   "metadata": {},
   "source": [
    "A key problem with the headlines data is that it was collected by scrapping online news sources, keyword matching and then feeding the keyword matches to Google Gemini for labelling. Everything not keyword matched is assumed to be a non-risk headline. Yet this likely causes us to have many false negatives as a result of the keyword matching process missing risk headlines. To tackle this problem, a regression model for each language (Spanish and Portuguese) is trained on half of the data at a time to generate predictions for the other half's non-risk headlines. A percentage of low probability headlines from each half is kept, eliminating many false negatives and tackling the class imbalance problem (see notebook 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df94d6-83db-4396-8697-9615ce4ebf8a",
   "metadata": {},
   "source": [
    "## Read-in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616ac5e-0973-4b72-8a57-4765133e4f53",
   "metadata": {},
   "source": [
    "Seperate dataframes are created for each language (Spanish & Portuguese). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a81d4ae-639e-4e61-802b-7b339ca3a8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.8K Total headlines\n",
      "69.3K Spanish headlines\n",
      "13.9K Portuguese headlines\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read-in data\n",
    "df = pd.read_csv('../Data/original_headlines.csv', encoding='utf-8')\n",
    "print(str(round(len(df)/1000, 1)) + 'K Total headlines')\n",
    "\n",
    "# include only spanish \n",
    "spanish_df = df[df.country.isin(['Argentina', 'Colombia', 'Mexico'])].reset_index(drop=True)\n",
    "print(str(round(len(spanish_df)/1000, 1)) + 'K Spanish headlines')\n",
    "\n",
    "# include only portuguese \n",
    "portuguese_df = df[df.country == 'Brazil'].reset_index(drop=True)\n",
    "print(str(round(len(portuguese_df)/1000, 1)) + 'K Portuguese headlines')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb259b58-b2bb-41d5-9e8f-16cd6daffba4",
   "metadata": {},
   "source": [
    "## Remove duplicates & thumbnails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a3559-f4ba-47aa-9288-51ce2f79e452",
   "metadata": {},
   "source": [
    "Headlines containing the word thumbnail are normally videos which cannot be scraped. Many of these with a similar format in the non-risk headlines data add little value in terms of variety and therefore are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa294472-f20c-4ad3-bc71-c140c9036403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "spanish_df.drop_duplicates(subset='headline', inplace=True)\n",
    "portuguese_df.drop_duplicates(subset='headline', inplace=True)\n",
    "\n",
    "# remove thumbnails\n",
    "spanish_df = spanish_df[~spanish_df['headline'].str.lower().str.contains('thumbnail', na=False)]\n",
    "portuguese_df = portuguese_df[~portuguese_df['headline'].str.lower().str.contains('thumbnail', na=False)]\n",
    "\n",
    "# removes english headlines from the spanish dataset\n",
    "spanish_df = spanish_df.loc[~spanish_df.website.isin(['Colombia Reports'])]\n",
    "\n",
    "# reset index\n",
    "spanish_df.reset_index(drop=True, inplace=True)\n",
    "portuguese_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a4cfdf-febc-45df-9704-eb429daa4e94",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77409c-89c6-455f-89cd-60c73b8cd091",
   "metadata": {},
   "source": [
    "The text is subjected to common cleaning techniques to reduce dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2156dd2c-0fd0-40f3-b6b4-078b8235a199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jack-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "spanish_stop_words = set(stopwords.words('Spanish'))\n",
    "portuguese_stop_words = set(stopwords.words('Portuguese'))\n",
    "\n",
    "# common text cleaning techniques\n",
    "def clean_text(text, language):\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation + '¡¿'))\n",
    "\n",
    "    if language=='Spanish':\n",
    "        text = ' '.join([word for word in text.split() if word not in spanish_stop_words])\n",
    "    elif language=='Portuguese':\n",
    "        text = ' '.join([word for word in text.split() if word not in portuguese_stop_words])\n",
    "    \n",
    "    return text\n",
    "\n",
    "spanish_df['headline'] = [clean_text(x, 'Spanish') for x in spanish_df['headline']]\n",
    "portuguese_df['headline'] = [clean_text(x, 'Portuguese') for x in portuguese_df['headline']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e6d8f0-8a8d-45bb-b234-dcda29e3c4c2",
   "metadata": {},
   "source": [
    "## Put aside data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7839b17f-75ef-4dc8-851f-87ea9cda453a",
   "metadata": {},
   "source": [
    "A random percentage of the data from each dataframe is put aside. It is useful to this now because if not we may end up evaluating on an artificially easy dataset in which most of the more difficult edge cases for non-risk headlines have been removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b46b6d-e836-42cc-8220-0d5699ff9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "# returns the main dataframe and a random sample to be put aside for evaluation\n",
    "def put_aside_random_percent(df, percent):\n",
    "    indices = list(df.index)\n",
    "    dividor = int(100 / percent)\n",
    "    sample_size = int(np.floor(len(indices)/dividor))\n",
    "    random_sample = random.sample(indices, sample_size)\n",
    "    put_aside_headlines = df.iloc[random_sample,:].reset_index(drop=True)\n",
    "    df = df.loc[~df.index.isin(random_sample)].reset_index(drop=True)\n",
    "    return df, put_aside_headlines\n",
    "\n",
    "spanish_df_post_sample, spanish_put_aside_df = put_aside_random_percent(spanish_df, 10)\n",
    "portuguese_df_post_sample, portuguese_put_aside_df = put_aside_random_percent(portuguese_df, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cde4337-0ee8-441c-ad72-c0a22eff56f6",
   "metadata": {},
   "source": [
    "## Split dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad3760-0778-4cd1-a8d4-56e4afc09132",
   "metadata": {},
   "source": [
    "Each dataframe is randomly split into two sets so a model can be trained on each set and used to predict headlines for the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac502d8e-2ee1-4945-adbb-08f875793708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split a dataframe into 2 equal size groups \n",
    "def split_dataframes(df):\n",
    "    population = list(range(len(df)))\n",
    "    half_headlines = int(np.floor(len(population) / 2))\n",
    "    random_samples = random.sample(population, half_headlines)\n",
    "    return df.loc[random_samples,:].reset_index(drop=True), df.loc[~df.index.isin(random_samples), :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827ab7a5-0013-4c5a-9211-43c3a4740fbb",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a83f72d-10e8-4d8c-ad4c-2ac7036bec22",
   "metadata": {},
   "source": [
    "A model is fit using TF-IDF vectors and logistic regression. A regression model is used to obtain probabilities so that the classification threshold can be easily varied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a650657-5a11-4fa9-81cb-8563d834430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# vectorizes data and fits a model \n",
    "def fit_model(df):\n",
    "    X, y = df.headline, [int(pd.notna(x)) for x in df.risk_type]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_tfidf = vectorizer.fit_transform(X)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(train_tfidf, y)\n",
    "    return vectorizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aab4d7d-7f58-4536-acd3-c63e974f3da4",
   "metadata": {},
   "source": [
    "## Generate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ed46e-0ff1-4bb7-83fd-c11e7810b208",
   "metadata": {},
   "source": [
    "Predictions from one half of each dataset are added to the other half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c962ca39-2012-4ba9-8924-249aa61d0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns predictions as binary decisions and probabilities\n",
    "def predict_headlines(df, vectorizer, model):\n",
    "    tfidf_vectors = vectorizer.transform(df.headline)\n",
    "    y_preds = model.predict(tfidf_vectors)\n",
    "    y_pred_prob = [np.mean(model.predict_proba(x)[:, 1]) for x in tfidf_vectors]\n",
    "    return y_preds, y_pred_prob\n",
    "\n",
    "# adds the predictions for each half of the data to their respective dfs\n",
    "def add_predictions_to_df(primary_df, secondary_df):\n",
    "    vectorizer, model = fit_model(secondary_df)\n",
    "    y_preds, y_pred_prob = predict_headlines(primary_df, vectorizer, model)\n",
    "    primary_df['y_pred'], primary_df['y_pred_prob'] = y_preds, y_pred_prob\n",
    "    primary_df.sort_values('y_pred_prob', ascending=False, inplace=True)\n",
    "    return primary_df\n",
    "\n",
    "# returns a headlines dataframe along with their predictions for veiwing\n",
    "# this is useful so we can set appropriate upper and lower limits for the \n",
    "# slice of low probability non-risk headlines we will select later on\n",
    "def view_headline_preds(df, language):\n",
    "    df_1, df_2 = split_dataframes(df) \n",
    "    df_1, df_2 = add_predictions_to_df(df_1, df_2), add_predictions_to_df(df_2, df_1)\n",
    "    #df_1, df_2 =  drop_non_risk_headlines(df_1, language), drop_non_risk_headlines(df_2, language)\n",
    "    return df_1, df_2\n",
    "\n",
    "spanish_df_1, spanish_df_2 = view_headline_preds(spanish_df_post_sample, 'Spanish')\n",
    "portuguese_df_1, portuguese_df_2 = view_headline_preds(portuguese_df_post_sample, 'Portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860a16e5-01d9-4381-82f2-e8cbbf17f617",
   "metadata": {},
   "source": [
    "## Find false negatives threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8efb4-5613-4d93-be69-ebc2e01951a7",
   "metadata": {},
   "source": [
    "The threshold at which false negatives are no longer common is located and used for the upper limits in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c34fd48-344e-48e3-bdb8-7a9f97de61f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Non-risk headlines: 23816\n",
      "\n",
      "13088:   molestia taxis “piratas” tizimín\n",
      "26832:   atención secundarios sólo tres días clases semana\n",
      "14649:   infraestructura vial portuaria dolor cabeza comercio exterior\n",
      "25893:   casi duplican casos hepatitis c coahuila 102 positivosunos 80 corresponden hombres 22 mujeres enero 13 julio 2024 casi duplicó cifra personas diagnosticadas hepatitis c coahuila respecto mismo periodo año pasado el…\n",
      "16889:   lamenta gobernadora muerte joven paola bañuelos estatal jueves 11 julio\n",
      "2384:   carlos loret mola timoratos\n",
      "10364:   reconocimiento cantante chalo botero día manzanareño\n",
      "1694:   regularán uso celulares escuelas buscan promover mejor concentracióna partir próximo ciclo escolar 20242025 iniciará control uso teléfonos celulares salones clase escuelas públicas educación básica durango partir próximo ciclo escolar 20242025 iniciará control uso teléfonos celulares salones clase escuelas públicas educación básica estado…\n",
      "5459:   joven picado dos veces alacrán dormía quedó internado terapia vida corre peligro\n",
      "12396:   anuncia coneculta convocatoria publicaciones 2025\n",
      "\n",
      "Non-risk headlines: 4616\n",
      "\n",
      "653:   moradora mc donalds leblon diz perseguida detalha agressão física pegou pescoço\n",
      "5452:   01 hábitos casamento heloisa périssé recorda experiência transar cem dias seguidos\n",
      "4672:   10 bahia “perdi dois empregos causa dele” diz exnamorada advogado preso extorsão ameaça stalking\n",
      "3780:   07 mercado iaô requalificação praça saveiros galpão artes entregue ribeira\n",
      "2830:   igarassu prefeita elcione governadora raquel unem forças aceleram obras cidade\n",
      "3671:   aliança contra fome pronta receber adesões\n",
      "79:   polícia encontra ossada advogada desaparecida 2 anos rs nada pode feito lamentam pais\n",
      "2869:   01 federação conversas mostram participação capitão pm bahia esquema facções diz pf\n",
      "582:   paulo thiago brennand transferido presídio famosos tremembé\n",
      "354:   anp jogador esperando entrar campo sobre hidrogênio diz diretorgeral\n"
     ]
    }
   ],
   "source": [
    "# this function shows the non-risk labelled headlines ordered\n",
    "# from the lowest to highest probability score. An ml practitioner \n",
    "# can use this to find the appropriate threshold for which \n",
    "# there are no longer many false negatives in the data...\n",
    "def view_nonrisk_highest_rows(df, start, end):\n",
    "    temp_df = df.loc[pd.isna(df.risk_type)].sort_values('y_pred_prob')\n",
    "    print()\n",
    "    print('Non-risk headlines: ' + str(len(temp_df)))\n",
    "    print()\n",
    "    selected_index_df = temp_df.loc[pd.isna(temp_df.risk_type)].iloc[start:end, :]\n",
    "    for i in range(len(selected_index_df)):\n",
    "        print(str(selected_index_df.index[i]) + ':   ' + selected_index_df.headline.values[i])\n",
    "    percent = end / len(temp_df)\n",
    "    return percent\n",
    "    \n",
    "spanish_percent = view_nonrisk_highest_rows(spanish_df_1, 7990, 8000)\n",
    "portuguese_percent = view_nonrisk_highest_rows(portuguese_df_1, 1990, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa88004-8879-4db2-b99e-1e532e355adf",
   "metadata": {},
   "source": [
    "## Drop headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9044d4-137d-4f8a-a3e7-78340defc58c",
   "metadata": {},
   "source": [
    "All non-risk headlines above the threshold above are dropped, eliminating many false negatives and tackling the class imbalance problem (see notebook 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f024a71-a4b9-4064-a061-10fea3991bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops a number of non-risk headlines based on their prediction scores\n",
    "def drop_non_risk_headlines(df, percent, language):   \n",
    "    non_risk_df = df.loc[pd.isna(df.risk_type)]\n",
    "    risk_df = df.loc[~pd.isna(df.risk_type)]\n",
    "    lower_limit, upper_limit = 0, int(np.floor(len(non_risk_df) * percent))\n",
    "    low_score_non_risk = non_risk_df.iloc[(len(non_risk_df)-upper_limit):(len(non_risk_df)-lower_limit),:]\n",
    "    return pd.concat([risk_df, low_score_non_risk])\n",
    "\n",
    "# creates a filtered dataframe combining both halfs of the data \n",
    "# after dropping headlines with high predictions\n",
    "def create_filtered_df(df_1, df_2, percent, language):\n",
    "    df_1, df_2 =  drop_non_risk_headlines(df_1, percent, language), drop_non_risk_headlines(df_2, percent, language)\n",
    "    return pd.concat([df_1, df_2])\n",
    "\n",
    "filtered_spanish_df = create_filtered_df(spanish_df_1, spanish_df_2, spanish_percent, language='Spanish')\n",
    "filtered_portuguese_df = create_filtered_df(portuguese_df_1, portuguese_df_2, portuguese_percent, language='Portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7736f3-fa01-494f-97b0-c56ae9bf9e9b",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef542219-483e-4df0-b305-1795bce8e170",
   "metadata": {},
   "source": [
    "Creates a train test split for a given dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb3ae042-787f-4062-a379-53c4c7965c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# returns a train test split\n",
    "def split_data(df, test_size=0.25):\n",
    "    X = df.headline\n",
    "    y = [int(pd.notna(x)) for x in df.risk_type]\n",
    "    return train_test_split(X, y, test_size=test_size, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb49fe-5010-4b12-bd75-c8ba47173e18",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ea452-06c4-4599-b55f-5f3848fba6c5",
   "metadata": {},
   "source": [
    "Prints the accuracy and classification report for a given model on a given set of headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6738b006-d7ac-46ce-b84d-659a6fdae24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# evaluates the model's performance and prints the results\n",
    "def evaluate_model(model, X_test_tfidf, y_test):\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    y_pred_prob = model.predict_proba(X_test_tfidf)[:, 1] \n",
    "    print(\"Accuracy:\", round(accuracy_score(y_test, y_pred),3))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2774c43-ef02-4c6d-970e-f1abfaf243fc",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b77f701-1521-46cb-856f-0dfe1abfa4bd",
   "metadata": {},
   "source": [
    "Trains a model on a refined dataset and evaluates it on set aside data. As we can see, the recall for risk headlines (the main business objective of this project) has improved dramatically for both languages compared to that of notebook 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55d5cfb4-7b65-44e0-bafe-fc1e01327d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Spanish ***\n",
      "\n",
      "6.11K put aside headlines\n",
      "Accuracy: 0.831\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.82      0.89      5291\n",
      "           1       0.44      0.88      0.58       818\n",
      "\n",
      "    accuracy                           0.83      6109\n",
      "   macro avg       0.71      0.85      0.74      6109\n",
      "weighted avg       0.91      0.83      0.85      6109\n",
      "\n",
      "\n",
      "\n",
      "*** Portuguese ***\n",
      "\n",
      "1.32K put aside headlines\n",
      "Accuracy: 0.786\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.77      0.85      1032\n",
      "           1       0.51      0.85      0.63       291\n",
      "\n",
      "    accuracy                           0.79      1323\n",
      "   macro avg       0.73      0.81      0.74      1323\n",
      "weighted avg       0.85      0.79      0.80      1323\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluates a filtered dataset against new headlines\n",
    "def check_results(train_df, put_aside_df, language):\n",
    "    print()\n",
    "    print('*** ' + language + ' ***')\n",
    "    print()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_data(train_df, test_size=0.01)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    print(str(round(len(put_aside_df)/1000, 2)) + 'K put aside headlines')\n",
    "    \n",
    "    X_test_tfidf = vectorizer.transform(put_aside_df.headline)\n",
    "    y_test = [int(pd.notna(x)) for x in put_aside_df.risk_type]\n",
    "    \n",
    "    evaluate_model(model, X_test_tfidf, y_test)\n",
    "\n",
    "check_results(filtered_spanish_df, spanish_put_aside_df, language='Spanish')\n",
    "check_results(filtered_portuguese_df, portuguese_put_aside_df, language='Portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292a000c-a525-4bf6-a3b0-4d260df1befb",
   "metadata": {},
   "source": [
    "## Repeat the process..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c771382-2c0b-4443-b996-8de07cd2f247",
   "metadata": {},
   "source": [
    "Given that the process above improved the model, it stands to reason that repeating it with predictions based on the newly refined dataset could improve the model even further. The results below show that although the overall accuracy for both models declined (likely because we are dropping badly needed data), the most important metric from a business perspective (risk headlines recall) went up in both datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88afb43e-9984-4efe-a429-2edaa271d5e0",
   "metadata": {},
   "source": [
    "### Generate new predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d284b06d-aa56-4f12-94a9-367e6e1401e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resets the indices\n",
    "filtered_spanish_df.reset_index(drop=True, inplace=True)\n",
    "filtered_portuguese_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# creates additional dataframes with new predictions based on a model trained on the newly filtered dataframe\n",
    "spanish_df_3, spanish_df_4 = view_headline_preds(filtered_spanish_df, 'Spanish')\n",
    "portuguese_df_3, portuguese_df_4 = view_headline_preds(filtered_portuguese_df, 'Portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c6628-8031-4e03-962d-d0d12f6f0a56",
   "metadata": {},
   "source": [
    "### Find new false negatives threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a62d713-ef4e-4038-8fd6-348af14f8db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Non-risk headlines: 8034\n",
      "\n",
      "8963:   rubén solís anuncia obras fpytch arranque semestre 0403\n",
      "1968:   penaltis premian fe uruguay sentencian brasil\n",
      "8675:   polémica presunto maltrato ensayos sanjuanero\n",
      "2447:   donald trump reúne benjamin netanyahu\n",
      "7941:   acuerdo culpabilidad marca fin saga legal assange pasó cinco años cárcel británica alta seguridad\n",
      "9559:   mary bianco fundadoras madres cumpliría 100 años\n",
      "2090:   incendios arrasan 1200 hectáreas bosques argentina\n",
      "2139:   contenedores 6 características contenedores basura comunitarios\n",
      "8481:   regidores tizimín irían cárcel desacato\n",
      "2306:   rodolfo hernández internado uci esposa dio detalles batalla cáncer\n",
      "\n",
      "Non-risk headlines: 2000\n",
      "\n",
      "939:   bolsonarista ameaçou colapsar sistema 8 janeiro virá ré processo stf 1h agência estado\n",
      "781:   alexandre moraes determina facebook cancele conta anderson torres hackeada\n",
      "762:   professor agredido aluno 17 anos mataleão dentro sala aula paraná\n",
      "884:   faz home office direito trabalhar onde quiser inclusive viagens\n",
      "3125:   coronel ulysses garante apoio mulheres empreendedoras acre\n",
      "2687:   aborto saidinhas marco temporal embate stf congresso vai além porte maconha\n",
      "763:   olimpíadas tão importantes entenda\n",
      "1052:   sindicato policiais descarta monitorar conversas lessa\n",
      "3154:   vendas varejo crescem 18 rs maio ante abril revela ibge\n",
      "3044:   justiçastj autoriza aborto legal negado adolescente 13 anos 28ª semana gestação garota tenta procedimento desde 18ª\n"
     ]
    }
   ],
   "source": [
    "# this function shows the non-risk labelled headlines ordered\n",
    "# from the lowest to highest probability score. An ml practitioner \n",
    "# can use this to find the appropriate threshold for which \n",
    "# there are no longer many false negatives in the data...\n",
    "new_spanish_percent = view_nonrisk_highest_rows(spanish_df_4, 7001, 7011)\n",
    "new_portuguese_percent = view_nonrisk_highest_rows(portuguese_df_4, 1681, 1691)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929534e-1f22-43eb-be85-b9f298f672c1",
   "metadata": {},
   "source": [
    "### Drop headlines and evaluate new results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11e3d9ce-c862-4330-9df9-02dcf4a714a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Spanish ***\n",
      "\n",
      "6.11K put aside headlines\n",
      "Accuracy: 0.772\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.75      0.85      5291\n",
      "           1       0.36      0.92      0.52       818\n",
      "\n",
      "    accuracy                           0.77      6109\n",
      "   macro avg       0.67      0.84      0.69      6109\n",
      "weighted avg       0.90      0.77      0.81      6109\n",
      "\n",
      "\n",
      "\n",
      "*** Portuguese ***\n",
      "\n",
      "1.32K put aside headlines\n",
      "Accuracy: 0.701\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.65      0.77      1032\n",
      "           1       0.42      0.88      0.56       291\n",
      "\n",
      "    accuracy                           0.70      1323\n",
      "   macro avg       0.68      0.77      0.67      1323\n",
      "weighted avg       0.83      0.70      0.73      1323\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create new filtered dfs with fewer non-risk headlines\n",
    "new_filtered_spanish_df = create_filtered_df(spanish_df_3, spanish_df_4, new_spanish_percent, language='Spanish')\n",
    "new_filtered_portuguese_df = create_filtered_df(portuguese_df_3, portuguese_df_4, new_portuguese_percent, language='Portuguese')\n",
    "\n",
    "# evaluates the results\n",
    "check_results(new_filtered_spanish_df, spanish_put_aside_df, language='Spanish')\n",
    "check_results(new_filtered_portuguese_df, portuguese_put_aside_df, language='Portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe0e2dc-cd3e-4a9c-b3ec-fdfa04b693b6",
   "metadata": {},
   "source": [
    "## Save dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546fb10f-79cf-4f2d-a7c5-85c922d09976",
   "metadata": {},
   "source": [
    "Finally, the refined dataframes are saved as CSV files for further use in additional notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f609336-8ac0-428a-9161-7bf3b6587af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_filtered_spanish_df.to_csv('../Data/spanish_df.csv', index=False)\n",
    "new_filtered_portuguese_df.to_csv('../Data/portuguese_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
